{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Что такое обучение?\n",
    "\n",
    "Компьютеры считывают данные, как мы видели в блокнотах 1 и 2. Затем мы можем создавать функции, которые моделируют эти данные для принятия решений, как мы видели в блокнотах 3 и 5. Но как убедиться, что модель действительно соответствует данным?\n",
    "\n",
    "В последнем блокноте мы увидели, что мы можем поиграться с параметрами нашей функции, определяющими модель, чтобы уменьшить функцию потерь. Однако мы не хотим сами выбирать параметры модели. Самостоятельный выбор параметров работает *достаточно хорошо*, когда мы имеем простую модель и всего несколько точек данных, но для более подробных моделей и больших наборов данных такой номер не пройдёт. Вместо этого мы хотим, чтобы наша машина *изучала* параметры, которые соответствуют модели, без необходимости самим манипулировать параметрами. В этом блокноте мы поговорим об «обучении» в **ML**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Мотивация: Подгонка параметров вручную\n",
    "\n",
    "Давайте вернемся к нашему примеру подбора параметров из блокнота 3. Напомним, что мы смотрели на то, может ли количество зеленого на фотографиях различать яблоко и банан, и использовали сигмовидную функцию для моделирования нашего выбора «яблоко или банан». используя количество зеленого цвета в изображении."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Pkg\n",
    "Pkg.add(\"Plots\")\n",
    "Pkg.add(\"Images\")\n",
    "Pkg.add(\"Interact\")\n",
    "Pkg.add(\"Statistics\")\n",
    "Pkg.add(\"ImageMagick\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Plots; gr()\n",
    "using Images; using Interact\n",
    "using Statistics\n",
    "\n",
    "σ(x,w,b) = 1 / (1 + exp(-w*x+b))\n",
    "\n",
    "apple =  load(\"data/10_100.jpg\")\n",
    "banana = load(\"data/104_100.jpg\")\n",
    "apple_green_amount =  mean(Float64.(green.(apple)))\n",
    "banana_green_amount = mean(Float64.(green.(banana)));\n",
    "\n",
    "@manipulate for w in -10:0.01:30, b in 0:0.1:30\n",
    "    \n",
    "    plot(x->σ(x,w,b), 0, 1, label=\"Model\", legend = :topleft, lw=3)\n",
    "    scatter!([apple_green_amount],  [0.0], label=\"Apple\")\n",
    "    scatter!([banana_green_amount], [1.0], label=\"Banana\")\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Интуитивно понятно, как вы настраивали ползунки так, чтобы модель отправляла яблоки в 0 и бананы в 1? Скорее всего, вы сделали следующее:\n",
    "\n",
    "**Немного переместить ползунки, посмотрить, движется ли кривая в правильном направлении, и если это так, продолжать.**\n",
    "\n",
    "Для машины «обучение» - это тот же процесс, переведенный в математику!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## «Обучение подталкиванием»: процесс спуска\n",
    "\n",
    "Давайте начнем формализовать эту идею. Чтобы подтолкнуть кривую в «правильном направлении», нам нужно измерить «насколько правильна» и «насколько неправильна» модель. Когда мы переводим идею «правильного направления» в математику, мы получаем **функцию потерь**, `L (w, b)`, как мы видели в блокноте 5. \n",
    "\n",
    "Мы говорим, что функция достигает минимума, когда модель `σ(x, w, b)` работает лучше всего. Теперь мы хотим создать функцию потерь, которая будет минимизирована, когда яблоко находится в точке «0», а банан в точке «1». Если данные (количество зеленого цвета) для нашего яблока равны $ x_1 $, то наша модель выведет $ σ (x_1, w, b) $ для нашего яблока.\n",
    "\n",
    "Итак, мы хотим, чтобы разница $ 0 - σ (x_1, w, b) $ была небольшой. Точно так же, если наши данные для нашего банана (количество зеленого банана) составляет $ x_2 $, мы хотим, чтобы разница в $ 1 - σ (x_2, w, b) $ была небольшой. Чтобы создать нашу функцию потерь, давайте сложим квадраты разности выходных данных модели от желаемых выходных данных для яблока и банана. Мы получаем\n",
    "\n",
    "$$ L(w,b) = (0 - σ(x_1, w, b))^2 + (1 - σ(x_2, w, b))^2. $$\n",
    "\n",
    "$L(w, b)$ имеет минимум когда достигает `0` для яблока и` 1` для банана, и, таким образом, стоимость является самой низкой, когда модель «верна». \n",
    "\n",
    "Мы можем визуализировать эту функцию, построив ее в 3D с помощью функции «поверхность»."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotly()\n",
    "# gr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L(w, b) = (0 - σ(apple_green_amount,w,b))^2 + (1 - σ(banana_green_amount,w,b))^2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_range = 10:0.1:13\n",
    "b_range = 0:1:20\n",
    "\n",
    "L_values = [L(w,b) for b in b_range, w in w_range]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@manipulate for w in w_range, b in b_range\n",
    "    p1 = surface(w_range, b_range, L_values, xlabel=\"w\", ylabel=\"b\", cam=(70,40), cbar=false, leg=false)\n",
    "    scatter!(p1, [w], [b], [L(w,b)+1e-2], markersize=5, color = :blue)\n",
    "    p2 = plot(x->σ(x,w,b), 0, 1, label=\"Model\", legend = :topleft, lw=3)\n",
    "    scatter!(p2, [apple_green_amount],  [0.0], label=\"Apple\", markersize=10)\n",
    "    scatter!(p2, [banana_green_amount], [1.0], label=\"Banana\", markersize=10, xlim=(0,1), ylim=(0,1))\n",
    "    plot(p1, p2, layout=(2,1))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Синий шар на трехмерном графике показывает текущий выбор параметров, обозначенный как `(w, b)`. Ниже 3D-графика показан 2D-график соответствующей модели с этими параметрами. Обратите внимание, что когда синий шар катится по склону, модель становится лучше. Наша функция потерь дает нам математическое представление о «холме», а процесс «обучения подталкиванием» просто катит мяч вниз по этому холму."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы сделать это математически, нам нужно знать, направление «спуска». Вспомните из матана или физики, что производная от `L` по отношению к` b` говорит вам, как изменяется `L` при изменении` b`. Таким образом, чтобы скатиться вниз, мы должны идти в направлении, где производная отрицательна (функция понижается) для каждого параметра. Этот спуск есть движение по направлению убывания **градиента**, $ \\nabla L $. Это означает, что «метод обучения с помощью подталкивания» можно перефразировать в математических терминах следующим образом:\n",
    "\n",
    "1. Рассчитать градиент\n",
    "2. Немного переместиться в направлении отрицательного градиента\n",
    "3. Повторить\n",
    "\n",
    "Этот процесс катания шара в направлении отрицательного градиента называется **градиентным спуском**; а по математически:\n",
    "\n",
    "$$p_{n+1} = p_n - \\eta \\nabla L(p_n).$$\n",
    "\n",
    "Здесь $ p_n $ представляет вектор текущих параметров $ (w, b) $; $ \\nabla L (p_n) $ - градиент функции потерь с учетом этих параметров. Мы начинаем с $ p_n $ и меняем его на $ \\eta \\nabla L (p_n) $, где $ \\eta $ - это маленький размер шага, который определяет, как далеко мы перемещаем параметры в направлении отрицательного градиента; Заметьте, что если вы зайдете слишком далеко, вы перешагнете минимум! В результате получается $ p_ {n + 1} $, новый вектор параметров.\n",
    "\n",
    "![image.png](data\\grad.gif)\n",
    "\n",
    "Если мы повторим этот процесс, то получим параметры, в которых модель правильно помечает яблоки как `0`, а бананы как` 1`. Когда это происходит, модель извлекла уроки из данных и затем может прочитать фотографии и сказать вам, являются ли они яблоками или бананами!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Упражнение 1\n",
    "\n",
    "Используйте следующие термины, чтобы заполнить предложения ниже. Термины могут использоваться более одного раза или не использоваться вообще:\n",
    "> градиент, функция потерь, производная, градиентный спуск, изучение.\n",
    "\n",
    "* Мы можем думать о _ (A) _ как о 1D версии _ (B) _.\n",
    "* Мы можем визуализировать _ (C) _ как холм.\n",
    "* В приведенном выше объяснении движение вниз по склону называется _ (D) _ и означает путешествие по _ (E) _.\n",
    "* Чтобы определить правильность модели, мы используем _ (F) _.\n",
    "* Когда наша программа может минимизировать _ (G) _ самостоятельно, мы говорим, что это _ (H) _.\n",
    "\n",
    "<br><br>\n",
    "\n",
    "A)<br>\n",
    "B)<br>\n",
    "C)<br>\n",
    "D)<br>\n",
    "E)<br>\n",
    "F)<br>\n",
    "G)<br>\n",
    "H)<br>"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": "9a9b9bcb-3526-4d4d-90e6-162fd70475cc",
   "lastKernelId": "2e43cc00-6b99-4f1a-9640-6daca030cc62"
  },
  "kernelspec": {
   "display_name": "Julia 1.4.1",
   "language": "julia",
   "name": "julia-1.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
