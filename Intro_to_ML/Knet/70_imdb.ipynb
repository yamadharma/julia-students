{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Модель классификации последовательностей для анализа настроений IMDB\n",
    "(c) Deniz Yuret, 2019\n",
    "* Задачи: Изучить структуру набора данных IMDB и обучить простой модели RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set display width, load packages, import symbols\n",
    "ENV[\"COLUMNS\"] = 72\n",
    "using Pkg; haskey(Pkg.installed(),\"Knet\") || Pkg.add(\"Knet\")\n",
    "using Statistics: mean\n",
    "using Knet: Knet, AutoGrad, RNN, param, dropout, minibatch, nll, accuracy, progress!, adam, save, load, gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0e-8"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Установите константы для модели и обучения\n",
    "EPOCHS=3          # Количество тренировочных эпох\n",
    "BATCHSIZE=64      # Количество экземпляров в мини-пакете\n",
    "EMBEDSIZE=125     # Размер вложения слова\n",
    "NUMHIDDEN=100     # Размер скрытого слоя\n",
    "MAXLEN=150        # максимальный размер последовательности слов, укороченные последовательности, усеченные более длинные\n",
    "VOCABSIZE=30000   # максимальный размер словаря, сохраняется наиболее частые 30K, сопоставляя остаток с токеном UNK\n",
    "NUMCLASS=2        # количество выходных классов\n",
    "DROPOUT=0.5       # Уровень отчисления\n",
    "LR=0.001          # Скорость обучения\n",
    "BETA_1=0.9        # Параметр оптимизации Адама\n",
    "BETA_2=0.999      # Параметр оптимизации Адама\n",
    "EPS=1e-08         # Параметр оптимизации Адама"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загрузка и просмотр данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "imdb"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "include(Knet.dir(\"data\",\"imdb.jl\"))   # определяет загрузчик IMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "imdb()\n",
       "\\end{verbatim}\n",
       "Load the IMDB Movie reviews sentiment classification dataset from https://keras.io/datasets and return (xtrn,ytrn,xtst,ytst,dict) tuple.\n",
       "\n",
       "\\section{Keyword Arguments:}\n",
       "\\begin{itemize}\n",
       "\\item url=https://s3.amazonaws.com/text-datasets: where to download the data (imdb.npz) from.\n",
       "\n",
       "\n",
       "\\item dir=Pkg.dir(\"Knet/data\"): where to cache the data.\n",
       "\n",
       "\n",
       "\\item maxval=nothing: max number of token values to include. Words are ranked by how often they occur (in the training set) and only the most frequent words are kept. nothing means keep all, equivalent to maxval = vocabSize + pad + stoken.\n",
       "\n",
       "\n",
       "\\item maxlen=nothing: truncate sequences after this length. nothing means do not truncate.\n",
       "\n",
       "\n",
       "\\item seed=0: random seed for sample shuffling. Use system seed if 0.\n",
       "\n",
       "\n",
       "\\item pad=true: whether to pad short sequences (padding is done at the beginning of sequences). pad\\_token = maxval.\n",
       "\n",
       "\n",
       "\\item stoken=true: whether to add a start token to the beginning of each sequence. start\\_token = maxval - pad.\n",
       "\n",
       "\n",
       "\\item oov=true: whether to replace words >= oov\\emph{token with oov}token (the alternative is to skip them). oov\\_token = maxval - pad - stoken.\n",
       "\n",
       "\\end{itemize}\n"
      ],
      "text/markdown": [
       "```\n",
       "imdb()\n",
       "```\n",
       "\n",
       "Load the IMDB Movie reviews sentiment classification dataset from https://keras.io/datasets and return (xtrn,ytrn,xtst,ytst,dict) tuple.\n",
       "\n",
       "# Keyword Arguments:\n",
       "\n",
       "  * url=https://s3.amazonaws.com/text-datasets: where to download the data (imdb.npz) from.\n",
       "  * dir=Pkg.dir(\"Knet/data\"): where to cache the data.\n",
       "  * maxval=nothing: max number of token values to include. Words are ranked by how often they occur (in the training set) and only the most frequent words are kept. nothing means keep all, equivalent to maxval = vocabSize + pad + stoken.\n",
       "  * maxlen=nothing: truncate sequences after this length. nothing means do not truncate.\n",
       "  * seed=0: random seed for sample shuffling. Use system seed if 0.\n",
       "  * pad=true: whether to pad short sequences (padding is done at the beginning of sequences). pad_token = maxval.\n",
       "  * stoken=true: whether to add a start token to the beginning of each sequence. start_token = maxval - pad.\n",
       "  * oov=true: whether to replace words >= oov*token with oov*token (the alternative is to skip them). oov_token = maxval - pad - stoken.\n"
      ],
      "text/plain": [
       "\u001b[36m  imdb()\u001b[39m\n",
       "\n",
       "  Load the IMDB Movie reviews sentiment classification dataset from\n",
       "  https://keras.io/datasets and return (xtrn,ytrn,xtst,ytst,dict)\n",
       "  tuple.\n",
       "\n",
       "\u001b[1m  Keyword Arguments:\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡\u001b[22m\n",
       "\n",
       "    •    url=https://s3.amazonaws.com/text-datasets: where to\n",
       "        download the data (imdb.npz) from.\n",
       "\n",
       "    •    dir=Pkg.dir(\"Knet/data\"): where to cache the data.\n",
       "\n",
       "    •    maxval=nothing: max number of token values to include.\n",
       "        Words are ranked by how often they occur (in the training\n",
       "        set) and only the most frequent words are kept. nothing\n",
       "        means keep all, equivalent to maxval = vocabSize + pad +\n",
       "        stoken.\n",
       "\n",
       "    •    maxlen=nothing: truncate sequences after this length.\n",
       "        nothing means do not truncate.\n",
       "\n",
       "    •    seed=0: random seed for sample shuffling. Use system seed\n",
       "        if 0.\n",
       "\n",
       "    •    pad=true: whether to pad short sequences (padding is done\n",
       "        at the beginning of sequences). pad_token = maxval.\n",
       "\n",
       "    •    stoken=true: whether to add a start token to the beginning\n",
       "        of each sequence. start_token = maxval - pad.\n",
       "\n",
       "    •    oov=true: whether to replace words >= oov\u001b[4mtoken with\n",
       "        oov\u001b[24mtoken (the alternative is to skip them). oov_token =\n",
       "        maxval - pad - stoken."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@doc imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Loading IMDB...\n",
      "└ @ Main /home/deniz/.julia/dev/Knet/data/imdb.jl:57\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  6.811008 seconds (29.27 M allocations: 1.493 GiB, 7.70% gc time)\n"
     ]
    }
   ],
   "source": [
    "@time (xtrn,ytrn,xtst,ytst,imdbdict)=imdb(maxlen=MAXLEN,maxval=VOCABSIZE);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000-element Array{Array{Int32,1},1}\n",
      "25000-element Array{Int8,1}\n",
      "25000-element Array{Array{Int32,1},1}\n",
      "25000-element Array{Int8,1}\n",
      "Dict{String,Int32} with 88584 entries\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(nothing, nothing, nothing, nothing, nothing)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "println.(summary.((xtrn,ytrn,xtst,ytst,imdbdict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1×150 LinearAlgebra.Adjoint{Int32,Array{Int32,1}}:\n",
       " 30000  30000  30000  30000  30000  …  1908  92  11  6  1  17  15  22"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Слова кодируются целыми числами\n",
    "rand(xtrn)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1×25000 LinearAlgebra.Adjoint{Int64,Array{Int64,1}}:\n",
       " 150  150  150  150  150  150  150  …  150  150  150  150  150  150"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Каждая последовательность слов дополняется или усекается до длины 150\n",
    "length.(xtrn)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "reviewstring (generic function with 2 methods)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Определяет функцию, которая может печатать фактические слова:\n",
    "imdbvocab = Array{String}(undef,length(imdbdict))\n",
    "for (k,v) in imdbdict; imdbvocab[v]=k; end\n",
    "imdbvocab[VOCABSIZE-2:VOCABSIZE] = [\"<unk>\",\"<s>\",\"<pad>\"]\n",
    "function reviewstring(x,y=0)\n",
    "    x = x[x.!=VOCABSIZE] # remove pads\n",
    "    \"\"\"$((\"Sample\",\"Negative\",\"Positive\")[y+1]) review:\\n$(join(imdbvocab[x],\" \"))\"\"\"\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive review:\n",
      "who definitely needed a hug these evil people capture the yokai and throw them into a red pit along with unwanted objects like <unk> and other mechanical things and these meld into one horribly violent robotic monsters whose only job is to kill takashi a young boy is the one to become their saviour alongside a red man dragon a turtle man and a river princess as well as a cute little creature that if it had been america they could have turned it into a cuddly toy and sold it at all good toy stores the lines are good especially the don't try this at home kids and other gems that bring a smile to your lips suspend belief and watch this with a child or on your own and enjoy though i must admit that the end was a wee bit sad and not necessarily so cheers <unk>\n"
     ]
    }
   ],
   "source": [
    "# Нажмите Ctrl-Enter, чтобы увидеть случайные абзацы:\n",
    "r = rand(1:length(xtrn))\n",
    "println(reviewstring(xtrn[r],ytrn[r]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1×25000 LinearAlgebra.Adjoint{Int8,Array{Int8,1}}:\n",
       " 1  2  2  1  1  1  1  1  2  2  1  …  1  2  2  1  1  2  1  1  1  2  2"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Вот метки: 1 = отрицательный, 2 = положительный are the labels: 1=negative, 2=positive\n",
    "ytrn'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Определяем модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct SequenceClassifier; input; rnn; output; pdrop; end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifier"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SequenceClassifier(input::Int, embed::Int, hidden::Int, output::Int; pdrop=0) =\n",
    "    SequenceClassifier(param(embed,input), RNN(embed,hidden,rnnType=:gru), param(output,hidden), pdrop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "function (sc::SequenceClassifier)(input)\n",
    "    embed = sc.input[:, permutedims(hcat(input...))]\n",
    "    embed = dropout(embed,sc.pdrop)\n",
    "    hidden = sc.rnn(embed)\n",
    "    hidden = dropout(hidden,sc.pdrop)\n",
    "    return sc.output * hidden[:,:,end]\n",
    "end\n",
    "\n",
    "(sc::SequenceClassifier)(input,output) = nll(sc(input),output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Эксперимент"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(390, 390)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtrn = minibatch(xtrn,ytrn,BATCHSIZE;shuffle=true)\n",
    "dtst = minibatch(xtst,ytst,BATCHSIZE)\n",
    "length.((dtrn,dtst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trainresults (generic function with 1 method)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Для проведения экспериментов\n",
    "function trainresults(file,model; o...)\n",
    "    if (print(\"Train from scratch? \"); readline()[1]=='y')\n",
    "        progress!(adam(model,repeat(dtrn,EPOCHS);lr=LR,beta1=BETA_1,beta2=BETA_2,eps=EPS))\n",
    "        Knet.save(file,\"model\",model)\n",
    "        Knet.gc() # To save gpu memory\n",
    "    else\n",
    "        isfile(file) || download(\"http://people.csail.mit.edu/deniz/models/tutorial/$file\",file)\n",
    "        model = Knet.load(file,\"model\")\n",
    "    end\n",
    "    return model\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.69312066f0, 0.69312423f0, 0.5135817307692307, 0.5096153846153846)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SequenceClassifier(VOCABSIZE,EMBEDSIZE,NUMHIDDEN,NUMCLASS,pdrop=DROPOUT)\n",
    "nll(model,dtrn), nll(model,dtst), accuracy(model,dtrn), accuracy(model,dtst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train from scratch? stdin> y\n",
      "1.53e-01  100.00%┣████████████████████┫ 1170/1170 [00:18/00:18, 64.14i/s]\n"
     ]
    }
   ],
   "source": [
    "# 2.51e-01  100.00%┣████████████████████┫ 1170/1170 [00:16/00:16, 75.46i/s]\n",
    "model = trainresults(\"imdbmodel113.jld2\",model);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.05890469f0, 0.38913542f0, 0.9833733974358975, 0.8548477564102565)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (0.059155148f0, 0.3877507f0, 0.9846153846153847, 0.8583733974358975)\n",
    "nll(model,dtrn), nll(model,dtst), accuracy(model,dtrn), accuracy(model,dtst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str2ids (generic function with 1 method)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictstring(x)=\"\\nPrediction: \" * (\"Negative\",\"Positive\")[argmax(Array(vec(model([x]))))]\n",
    "UNK = VOCABSIZE-2\n",
    "str2ids(s::String)=[(i=get(imdbdict,w,UNK); i>=UNK ? UNK : i) for w in split(lowercase(s))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative review:\n",
      "<s> this is an emperor's new clothes situation someone needs to say that's not a funny and original etc etc film that is an inferior film don't waste your money on it the film is trashy and the people in it are embarrassingly inferior trailer trash they are all too realistically only themselves they have no lines they don't act the american dream is not to create shoddy no quality films or anything else shoddy and of no quality it is to achieve something of quality and thereby success only people who are desperate to praise any film not made in hollywood it can't have been made in hollywood can it would try to any kind of quality to this film it's worse than ed woods another film about a film maker without standards these films shouldn't have been made and you shouldn't go see american movie\n",
      "\n",
      "Prediction: Negative\n"
     ]
    }
   ],
   "source": [
    "# Здесь мы видим прогнозы для случайных обзоров из тестового набора; нажмите Ctrl-Enter для примера:\n",
    "r = rand(1:length(xtst))\n",
    "println(reviewstring(xtst[r],ytst[r]))\n",
    "println(predictstring(xtst[r]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stdin> this was not a great movie\n",
      "\n",
      "Prediction: Negative\n"
     ]
    }
   ],
   "source": [
    "# Прежде чем пользователь может ввести свои отзывы и классифицировать их:\n",
    "println(predictstring(str2ids(readline(stdin))))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "julia.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Julia 1.1.0",
   "language": "julia",
   "name": "julia-1.1"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
